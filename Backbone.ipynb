{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow_addons.optimizers import SGDW, AdamW, AdaBelief\n",
    "from tensorflow.keras import mixed_precision\n",
    "from Losses import Semantic_loss_functions\n",
    "from DatasetUtils import Dataset\n",
    "from EvaluationUtils import MeanIoU, IoULoss\n",
    "from SegmentationModels import Residual_Unet\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "\n",
    "#mixed_precision.set_global_policy('mixed_float16') # -> can use larger batch size (double)\n",
    "\n",
    "BACKBONE = 'EfficientNet'\n",
    "BACKBONE_VERSION = 'B3'\n",
    "BACKBONE_NAME = BACKBONE + BACKBONE_VERSION\n",
    "\n",
    "MODEL_TYPE = str(sys.argv[1])\n",
    "MODEL_NAME = str(sys.argv[2])\n",
    "NUM_CLASSES = int(sys.argv[3])\n",
    "PREPROCESSING = BACKBONE\n",
    "EPOCHS = 60\n",
    "FILTERS = [16,32,64,128,256,512]\n",
    "INPUT_SHAPE = (1024, 2048, 3)\n",
    "BATCH_SIZE = 3\n",
    "ACTIVATION = 'leaky_relu'\n",
    "DROPOUT_RATE = 0.0\n",
    "DROPOUT_OFFSET = 0.02\n",
    "\n",
    "ignore_ids = [0,1,2,3,4,5,6,9,10,14,15,16,18,29,30]\n",
    "\n",
    "#######################################################################################\n",
    "data_path = ''  \n",
    "img_path = 'leftImg8bit_trainvaltest/leftImg8bit'\n",
    "label_path = 'gtFine_trainvaltest/gtFine'\n",
    "train_path = '/train'\n",
    "val_path = '/val'\n",
    "test_path = '/test'\n",
    "train = '/*'\n",
    "val = '/*'\n",
    "test = '/*'\n",
    "img_type = '/*.png'\n",
    "label_type = '/*_gtFine_labelIds.png'\n",
    "\n",
    "img_train_path = data_path + img_path + train_path + train + img_type\n",
    "img_val_path = data_path + img_path + val_path + val + img_type\n",
    "img_test_path = data_path + img_path + test_path + test + img_type\n",
    "\n",
    "label_train_path = data_path + label_path + train_path + train + label_type\n",
    "label_val_path =  data_path + label_path + val_path + val + label_type\n",
    "label_test_path = data_path + label_path + test_path + test + label_type\n",
    "\n",
    "# -------------------------------CALLBACKS---------------------------------------------------\n",
    "checkpoint_filepath = f'saved_models/{MODEL_TYPE}/{MODEL_NAME}'\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,                                           \n",
    "                                            save_weights_only=False,\n",
    "                                            monitor='val_MeanIoU',\n",
    "                                            mode='max',\n",
    "                                            save_best_only=True,\n",
    "                                            verbose=0)\n",
    "\n",
    "log_dir = f'Tensorboard_logs/{MODEL_TYPE}/{MODEL_NAME}'\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir,\n",
    "                                histogram_freq=0,\n",
    "                                write_graph=False,\n",
    "                                write_steps_per_second=False)\n",
    "\n",
    "# csv_logs_dir = f'CSV_logs/{MODEL_TYPE}/{MODEL_NAME}.csv'\n",
    "# os.makedirs(csv_logs_dir, exist_ok=True)\n",
    "# csv_callback = CSVLogger(csv_logs_dir)\n",
    "\n",
    "callbacks = [model_checkpoint_callback, tensorboard_callback]\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "train_ds = Dataset(NUM_CLASSES, 'train', PREPROCESSING, shuffle=True)\n",
    "train_ds = train_ds.create(img_train_path, label_train_path, BATCH_SIZE, use_patches=False, augment=False)\n",
    "\n",
    "val_ds = Dataset(NUM_CLASSES, 'validation', PREPROCESSING, shuffle=False)\n",
    "val_ds = val_ds.create(img_val_path, label_val_path, BATCH_SIZE, use_patches=False, augment=False)\n",
    "\n",
    "steps_per_epoch = 992\n",
    "MIN_LR = 1e-3\n",
    "MAX_LR = 7e-3\n",
    "lr_schedule = CyclicalLearningRate(initial_learning_rate=MIN_LR,\n",
    "                                            maximal_learning_rate=MAX_LR,\n",
    "                                            scale_fn=lambda x: 0.95**x,\n",
    "                                            step_size=4*steps_per_epoch,\n",
    "                                            )\n",
    "\n",
    "s = Semantic_loss_functions()\n",
    "# loss = s.dice_loss\n",
    "loss = IoULoss()\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "# optimizer = SGD(learning_rate=0.01,\n",
    "#                 momentum=0.9)\n",
    "\n",
    "if NUM_CLASSES==34:\n",
    "    ignore_class = ignore_ids\n",
    "else:\n",
    "    ignore_class = 19\n",
    "\n",
    "mean_iou = MeanIoU(NUM_CLASSES, name='MeanIoU', ignore_class=None)\n",
    "mean_iou_ignore = MeanIoU(NUM_CLASSES, name='MeanIoU_ignore', ignore_class=ignore_class)\n",
    "metrics = [s.jacard_coef, mean_iou]\n",
    "\n",
    "model = Residual_Unet(input_shape=INPUT_SHAPE,\n",
    "                      filters=FILTERS,\n",
    "                      num_classes=NUM_CLASSES,\n",
    "                      activation='leaky_relu',\n",
    "                      dropout_rate=DROPOUT_RATE,\n",
    "                      dropout_type='normal',\n",
    "                      scale_dropout=False,\n",
    "                      dropout_offset=DROPOUT_OFFSET,\n",
    "                      backbone_name=BACKBONE_NAME,\n",
    "                      freeze_backbone=True\n",
    "                      )\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# train model with backbone frozen\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks = callbacks,\n",
    "                    verbose = 1\n",
    "                    )\n",
    "\n",
    "\n",
    "# model = Residual_Unet(input_shape=INPUT_SHAPE,\n",
    "#                       filters=FILTERS,\n",
    "#                       num_classes=NUM_CLASSES,\n",
    "#                       activation='leaky_relu',\n",
    "#                       dropout_rate=DROPOUT_RATE,\n",
    "#                       dropout_type='normal',\n",
    "#                       scale_dropout=False,\n",
    "#                       dropout_offset=DROPOUT_OFFSET,\n",
    "#                       backbone_name=backbone_name,\n",
    "#                       freeze_backbone=False\n",
    "#                       )\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "# model.summary()\n",
    "\n",
    "# # REDEFINE BATCH SIZE, LEARNING RATE\n",
    "# optimizer = Adam(learning_rate=0.0005)\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# BATCH_SIZE = 1\n",
    "# train_ds = Dataset('train', preprocessing=backbone)\n",
    "# train_ds = train_ds.create(img_train_path, label_train_path, NUM_CLASSES, BATCH_SIZE, use_patches=False, augment=False)\n",
    "\n",
    "# val_ds = Dataset('validation', preprocessing=backbone)\n",
    "# val_ds = val_ds.create(img_val_path, label_val_path, NUM_CLASSES, BATCH_SIZE, use_patches=False, augment=False)\n",
    "\n",
    "# history = model.fit(train_ds,\n",
    "#                     validation_data=val_ds,\n",
    "#                     initial_epoch=EPOCHS,\n",
    "#                     epochs=FINAL_EPOCHS,\n",
    "#                     callbacks = callbacks,\n",
    "#                     verbose = 1\n",
    "#                     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
